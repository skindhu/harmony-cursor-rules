#!/usr/bin/env python3
"""
HarmonyOSç•Œé¢å¼€å‘æœ€ä½³å®è·µçˆ¬è™«
ä¸“é—¨é’ˆå¯¹åä¸ºå¼€å‘è€…å®˜ç½‘çš„SPAé¡µé¢è¿›è¡Œä¼˜åŒ–

ç”¨æ³•ï¼š
- é»˜è®¤è¿è¡Œï¼špython main.py
- è°ƒè¯•æ¨¡å¼ï¼špython main.py --debug  (ä¿å­˜HTMLæ–‡ä»¶)
"""

import asyncio
import os
import json
import time
from pathlib import Path
from typing import Dict, Any
from config import ConfigManager
from ai import ContentProcessor
from crawler import WebCrawler
from batch import BatchProcessor
from arkts_lint import ArkTSRulesExtractor


class SPACrawler:
    def __init__(self, config_manager: ConfigManager = None):
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
        if config_manager is None:
            config_manager = ConfigManager.from_command_line()
        self.config_manager = config_manager

        # ä»é…ç½®ç®¡ç†å™¨è·å–é…ç½®
        self.output_dir = self.config_manager.get_output_directory()
        self.debug = self.config_manager.is_debug_mode()

        # åˆå§‹åŒ–AIå†…å®¹å¤„ç†å™¨
        self.content_processor = ContentProcessor()
        if self.content_processor.is_api_available():
            print("âœ… AIå†…å®¹å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âš ï¸ AIå†…å®¹å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡AIå¤„ç†åŠŸèƒ½")

        # åˆå§‹åŒ–æ ¸å¿ƒçˆ¬è™«
        self.web_crawler = WebCrawler(
            config_manager=self.config_manager,
            content_processor=self.content_processor
        )
        print("âœ… æ ¸å¿ƒçˆ¬è™«æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        self.batch_processor = BatchProcessor(
            web_crawler=self.web_crawler,
            output_dir=self.output_dir
        )
        print("âœ… æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        # åˆå§‹åŒ–ArkTSè§„åˆ™æå–å™¨
        self.arkts_extractor = ArkTSRulesExtractor(
            web_crawler=self.web_crawler,
            gemini_api=self.content_processor.gemini_api,
            output_dir=self.output_dir
        )
        print("âœ… ArkTSè§„åˆ™æå–å™¨åˆå§‹åŒ–æˆåŠŸ")





    async def crawl_with_directory_structure(self, target_dir: Path, url: str, module_name: str, sub_module_name: str) -> Dict[str, Any]:
        """
        æŒ‰ç›®å½•ç»“æ„çˆ¬å–å¹¶ä¿å­˜æ–‡ä»¶

        Args:
            target_dir: ç›®æ ‡ç›®å½•
            url: ç›®æ ‡URL
            module_name: æ¨¡å—åç§°ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
            sub_module_name: å­æ¨¡å—ä¸­æ–‡åç§°

        Returns:
            Dict: çˆ¬å–ç»“æœ
        """
        return await self.web_crawler.crawl_with_directory_structure(
            target_dir=target_dir,
            url=url,
            module_name=module_name,
            sub_module_name=sub_module_name
        )



    async def crawl_spa_page(self, url: str, module_name: str = None) -> Dict[str, Any]:
        """
        çˆ¬å–SPAé¡µé¢ï¼Œç›´æ¥ä¿å­˜åŸå§‹HTMLå†…å®¹

        Args:
            url: ç›®æ ‡URL
            module_name: æ¨¡å—åç§°ï¼Œç”¨äºæ–‡ä»¶å‘½å
        """
        return await self.web_crawler.crawl_spa_page_legacy(
            url=url,
            module_name=module_name
        )



    async def crawl_all_harmony_modules(self, config_file: str = "harmony_modules_config.json"):
        """
        æ ¹æ®é…ç½®æ–‡ä»¶çˆ¬å–æ‰€æœ‰HarmonyOSæ¨¡å—

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        return await self.batch_processor.process_harmony_modules(config_file)

    async def integrate_best_practices(self, config_file: str = "harmony_modules_config.json"):
        """
        æ•´åˆæ¯ä¸ªä¸€çº§ç›®å½•ä¸­çš„æ‰€æœ‰æœ€ä½³å®è·µï¼Œç”Ÿæˆç¬¦åˆCursor Rulesæ ¼å¼çš„è§„èŒƒæ–‡ä»¶

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        return await self.batch_processor.integrate_all_best_practices(config_file)

    async def extract_arkts_rules(self) -> Dict[str, Any]:
        """
        æå–ArkTS Lintè§„åˆ™

        Returns:
            Dict: æå–ç»“æœ
        """
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        arkts_rules_file = self.output_dir / "final_cursor_rules" / "arkts-lint-rules.md"
        if arkts_rules_file.exists():
            print("ğŸ“‹ ArkTSè§„åˆ™æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–")
            return {
                "success": True,
                "message": "æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–",
                "output_file": str(arkts_rules_file),
                "skipped": True
            }

        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹æå–ArkTS Lintè§„åˆ™")
        print("="*60)

        # æ‰§è¡Œæå–
        result = await self.arkts_extractor.extract_arkts_rules_from_url()

        if result.get("success", False):
            print(f"âœ… ArkTSè§„åˆ™æå–æˆåŠŸï¼")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {result.get('output_file', 'N/A')}")
            print(f"ğŸ“Š æå–è§„åˆ™æ•°é‡: {result.get('rules_count', 0)}")
        else:
            print(f"âŒ ArkTSè§„åˆ™æå–å¤±è´¥")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        return result





async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = ConfigManager.from_command_line()

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = SPACrawler(config_manager)

    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    config_manager.print_startup_info()

    results = await crawler.crawl_all_harmony_modules()

    if results:
        successful_count = len([r for r in results if r.get("success")])
        total_count = len(results)

        print(f"\nğŸŠ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸç‡: {successful_count}/{total_count} ({successful_count/total_count*100:.1f}%)")
        print("\nâœ¨ æ‰€æœ‰HarmonyOSç•Œé¢å¼€å‘æœ€ä½³å®è·µå·²æ•´ç†å®Œæˆï¼")

        # æ‰§è¡Œæœ€ä½³å®è·µæ•´åˆï¼Œç”ŸæˆCursor Rules
        if successful_count > 0:
            integration_results = await crawler.integrate_best_practices()
            if integration_results:
                successful_integrations = len([r for r in integration_results if r['success']])
                print(f"\nğŸ¯ Cursor Rulesç”Ÿæˆå®Œæˆï¼æˆåŠŸæ•´åˆ {successful_integrations} ä¸ªä¸€çº§æ¨¡å—")
            else:
                print(f"\nâš ï¸ Cursor Rulesæ•´åˆè·³è¿‡æˆ–å¤±è´¥")

        # æå–ArkTS Lintè§„åˆ™
        arkts_result = await crawler.extract_arkts_rules()
        if arkts_result.get("success", False):
            if arkts_result.get("skipped", False):
                print(f"\nâ­ï¸ ArkTSè§„åˆ™æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–")
            else:
                print(f"\nğŸŠ ArkTSè§„åˆ™æå–å®Œæˆï¼")
                print(f"ğŸ“Š æå–äº† {arkts_result.get('rules_count', 0)} ä¸ªè§„åˆ™")
        else:
            print(f"\nâš ï¸ ArkTSè§„åˆ™æå–å¤±è´¥: {arkts_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    else:
        print("\nâŒ çˆ¬å–ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")

    # å¦‚æœéœ€è¦å•ç‹¬æµ‹è¯•æŸä¸ªURLï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š
    # test_url = "https://developer.huawei.com/consumer/cn/doc/best-practices/bpta-ui-dynamic-operations"
    # result = await crawler.crawl_spa_page(test_url, "test_module")
    # print(f"å•ä¸ªæµ‹è¯•ç»“æœ: {result}")


if __name__ == "__main__":
    asyncio.run(main())